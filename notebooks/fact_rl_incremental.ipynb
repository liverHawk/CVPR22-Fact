{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# FACT + RL による新セッション訓練\n",
    "\n",
    "このノートブックでは、訓練済みのベースモデル（session 0）を使って、新しいセッション（session 1以降）の訓練を強化学習（DQN）で行います。\n",
    "\n",
    "## 処理フロー\n",
    "1. ベースセッションで訓練済みのFACTモデルをロード\n",
    "2. 新セッションのfew-shotデータをロード\n",
    "3. DQNエージェントを使って新クラスを学習\n",
    "4. 既存クラスの忘却を抑えながら新クラスを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(f\"Using torch {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロジェクトルートとパスの設定\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / \"checkpoint\" / \"CICIDS2017_improved\"\n",
    "\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"CHECKPOINT_DIR: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロジェクト固有のインポート\n",
    "from dataloader.cicids2017.cicids2017 import CICIDS2017_improved\n",
    "from models.fact.Network import MYNET\n",
    "from utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータ設定\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# データセット設定（CICIDS2017_improvedのデフォルト）\n",
    "BASE_CLASS = 4\n",
    "NUM_CLASSES = 10\n",
    "WAY = 1  # 新セッションごとに追加されるクラス数\n",
    "SHOT = 5  # Few-shotサンプル数\n",
    "NUM_SESSIONS = 7  # インクリメンタルセッション数\n",
    "\n",
    "# 訓練するセッション番号（1以降）\n",
    "TARGET_SESSION = 1\n",
    "\n",
    "# RL設定\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 0.995\n",
    "TARGET_UPDATE = 10\n",
    "NUM_EPISODES = 500\n",
    "MAX_STEPS_PER_EPISODE = 100\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Target Session: {TARGET_SESSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-load-base-model-title",
   "metadata": {},
   "source": [
    "## 1. ベースモデルのロード\n",
    "\n",
    "session 0で訓練済みのFACTモデルをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-base-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル設定の作成（argsの代わり）\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset = \"CICIDS2017_improved\"\n",
    "        self.encoder = \"mlp\"  # or \"cnn1d\"\n",
    "        self.base_mode = \"ft_cos\"\n",
    "        self.new_mode = \"avg_cos\"\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.base_class = BASE_CLASS\n",
    "        self.device = DEVICE\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# FACTモデルの初期化\n",
    "base_model = MYNET(args, mode=args.base_mode)\n",
    "base_model = base_model.to(DEVICE)\n",
    "\n",
    "# ベースセッションの重みをロード\n",
    "checkpoint_path = CHECKPOINT_DIR / \"session0_max_acc.pth\"\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "    base_model.load_state_dict(checkpoint['params'])\n",
    "    print(f\"✓ Loaded base model from {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"⚠ Warning: Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"Using randomly initialized model. Train base session first with train.py\")\n",
    "\n",
    "base_model.eval()\n",
    "print(f\"Base model loaded on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-load-data-title",
   "metadata": {},
   "source": [
    "## 2. データのロード\n",
    "\n",
    "新セッションのfew-shotデータと、評価用の全クラスデータをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新セッションの訓練データ（few-shot）\n",
    "session_txt = DATA_DIR / \"index_list\" / \"CICIDS2017_improved\" / f\"session_{TARGET_SESSION}.txt\"\n",
    "train_dataset_new = CICIDS2017_improved(\n",
    "    root=str(DATA_DIR), \n",
    "    train=True, \n",
    "    index_path=str(session_txt),\n",
    "    normalize_method=\"moving_minmax\"\n",
    ")\n",
    "\n",
    "# 評価用：これまでに見たすべてのクラス（base + 新セッション）\n",
    "class_indices_cumulative = list(range(BASE_CLASS + TARGET_SESSION * WAY))\n",
    "test_dataset_all = CICIDS2017_improved(\n",
    "    root=str(DATA_DIR),\n",
    "    train=False,\n",
    "    index=class_indices_cumulative,\n",
    "    base_sess=False,\n",
    "    normalize_method=\"moving_minmax\"\n",
    ")\n",
    "\n",
    "print(f\"New session training samples: {len(train_dataset_new)}\")\n",
    "print(f\"New session classes: {np.unique(train_dataset_new.targets)}\")\n",
    "print(f\"Test dataset (all seen classes): {len(test_dataset_all)} samples\")\n",
    "print(f\"Test classes: {np.unique(test_dataset_all.targets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-rl-env-title",
   "metadata": {},
   "source": [
    "## 3. RL環境の定義\n",
    "\n",
    "新セッションのfew-shotデータを使って、分類タスクをRL環境として定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-rl-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotClassificationEnv:\n",
    "    \"\"\"Few-shot incremental learning用のRL環境\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, num_classes, max_steps=100):\n",
    "        self.features = torch.tensor(dataset.data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(dataset.targets, dtype=torch.long)\n",
    "        self.num_classes = num_classes\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.current_idx = 0\n",
    "        self.indices = None\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"エピソードの開始\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.indices = torch.randperm(len(self.features))\n",
    "        self.current_idx = 0\n",
    "        return self.features[self.indices[self.current_idx]].numpy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"アクション（クラス予測）を実行\"\"\"\n",
    "        true_label = self.labels[self.indices[self.current_idx]].item()\n",
    "        \n",
    "        # 報酬：正解なら+1、不正解なら-1\n",
    "        reward = 1.0 if action == true_label else -1.0\n",
    "        \n",
    "        self.current_step += 1\n",
    "        self.current_idx = (self.current_idx + 1) % len(self.features)\n",
    "        \n",
    "        done = self.current_step >= self.max_steps\n",
    "        \n",
    "        if done:\n",
    "            next_state = np.zeros_like(self.features[0].numpy())\n",
    "        else:\n",
    "            next_state = self.features[self.indices[self.current_idx]].numpy()\n",
    "        \n",
    "        info = {'true_label': true_label, 'correct': action == true_label}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "# 環境の作成\n",
    "env = FewShotClassificationEnv(\n",
    "    train_dataset_new, \n",
    "    num_classes=BASE_CLASS + TARGET_SESSION * WAY,\n",
    "    max_steps=MAX_STEPS_PER_EPISODE\n",
    ")\n",
    "print(f\"✓ Environment created with {env.num_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-dqn-title",
   "metadata": {},
   "source": [
    "## 4. DQNエージェントの定義\n",
    "\n",
    "ベースモデルのエンコーダを活用したDQNエージェントを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dqn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    \"\"\"ベースモデルのエンコーダを使うDQNエージェント\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, num_actions, freeze_encoder=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # エンコーダの凍結オプション\n",
    "        if freeze_encoder:\n",
    "            for param in self.base_model.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # 新しいQ値ヘッドを追加\n",
    "        self.q_head = nn.Sequential(\n",
    "            nn.Linear(base_model.num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ベースモデルのエンコーダで特徴抽出\n",
    "        with torch.no_grad() if hasattr(self, '_freeze_encoder') and self._freeze_encoder else torch.enable_grad():\n",
    "            features = self.base_model.encoder(x)\n",
    "        \n",
    "        # Q値の計算\n",
    "        q_values = self.q_head(features)\n",
    "        return q_values\n",
    "\n",
    "\n",
    "# DQNエージェントの初期化\n",
    "policy_net = DQNAgent(\n",
    "    base_model, \n",
    "    num_actions=BASE_CLASS + TARGET_SESSION * WAY,\n",
    "    freeze_encoder=False  # Trueにするとエンコーダを凍結\n",
    ").to(DEVICE)\n",
    "\n",
    "target_net = DQNAgent(\n",
    "    base_model,\n",
    "    num_actions=BASE_CLASS + TARGET_SESSION * WAY,\n",
    "    freeze_encoder=False\n",
    ").to(DEVICE)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "print(f\"✓ DQN Agent initialized\")\n",
    "print(f\"  - Policy net parameters: {sum(p.numel() for p in policy_net.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-train-title",
   "metadata": {},
   "source": [
    "## 5. 訓練ループ\n",
    "\n",
    "epsilon-greedy方策でDQNエージェントを訓練します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-train-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon):\n",
    "    \"\"\"epsilon-greedy方策でアクションを選択\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(policy_net.num_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.argmax(dim=1).item()\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"Replay bufferからサンプリングしてネットワークを更新\"\"\"\n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "    states = states.to(DEVICE)\n",
    "    actions = actions.to(DEVICE)\n",
    "    rewards = rewards.to(DEVICE)\n",
    "    next_states = next_states.to(DEVICE)\n",
    "    dones = dones.to(DEVICE)\n",
    "    \n",
    "    # 現在のQ値\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # 次状態の最大Q値（target network使用）\n",
    "    with torch.no_grad():\n",
    "        next_q = target_net(next_states).max(1)[0]\n",
    "        target_q = rewards + (1 - dones) * GAMMA * next_q\n",
    "    \n",
    "    # Loss計算と最適化\n",
    "    loss = F.mse_loss(current_q, target_q)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_agent(dataset, device=DEVICE):\n",
    "    \"\"\"エージェントの評価\"\"\"\n",
    "    policy_net.eval()\n",
    "    features = torch.FloatTensor(dataset.data).to(device)\n",
    "    labels = torch.LongTensor(dataset.targets).to(device)\n",
    "    \n",
    "    q_values = policy_net(features)\n",
    "    predictions = q_values.argmax(dim=1)\n",
    "    accuracy = (predictions == labels).float().mean().item()\n",
    "    \n",
    "    policy_net.train()\n",
    "    return accuracy\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練ループ\n",
    "epsilon = EPSILON_START\n",
    "episode_rewards = []\n",
    "episode_losses = []\n",
    "episode_accuracies = []\n",
    "\n",
    "print(f\"Starting training for {NUM_EPISODES} episodes...\\n\")\n",
    "\n",
    "for episode in tqdm(range(NUM_EPISODES), desc=\"Training\"):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_loss = []\n",
    "    \n",
    "    for step in range(MAX_STEPS_PER_EPISODE):\n",
    "        # アクション選択\n",
    "        action = select_action(state, epsilon)\n",
    "        \n",
    "        # 環境でステップ実行\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Replay bufferに保存\n",
    "        replay_buffer.push(state, action, reward, next_state, float(done))\n",
    "        \n",
    "        # ネットワーク更新\n",
    "        loss = optimize_model()\n",
    "        if loss is not None:\n",
    "            episode_loss.append(loss)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Epsilon decay\n",
    "    epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "    \n",
    "    # Target networkの更新\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    # ログ記録\n",
    "    episode_rewards.append(episode_reward)\n",
    "    avg_loss = np.mean(episode_loss) if episode_loss else 0\n",
    "    episode_losses.append(avg_loss)\n",
    "    \n",
    "    # 定期的に評価\n",
    "    if episode % 50 == 0 or episode == NUM_EPISODES - 1:\n",
    "        test_acc = evaluate_agent(test_dataset_all)\n",
    "        episode_accuracies.append((episode, test_acc))\n",
    "        print(f\"\\nEpisode {episode:3d} | Reward: {episode_reward:6.2f} | Loss: {avg_loss:.4f} | Epsilon: {epsilon:.3f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-eval-title",
   "metadata": {},
   "source": [
    "## 6. 最終評価\n",
    "\n",
    "訓練後のエージェントを評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-final-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終評価\n",
    "final_test_acc = evaluate_agent(test_dataset_all)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Final Test Accuracy (all seen classes): {final_test_acc:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# クラスごとの精度\n",
    "policy_net.eval()\n",
    "with torch.no_grad():\n",
    "    features = torch.FloatTensor(test_dataset_all.data).to(DEVICE)\n",
    "    labels = torch.LongTensor(test_dataset_all.targets)\n",
    "    predictions = policy_net(features).cpu().argmax(dim=1)\n",
    "    \n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for cls in sorted(np.unique(test_dataset_all.targets)):\n",
    "        cls_mask = labels == cls\n",
    "        cls_acc = (predictions[cls_mask] == labels[cls_mask]).float().mean().item()\n",
    "        cls_type = \"Base\" if cls < BASE_CLASS else f\"New (Session {(cls - BASE_CLASS) // WAY + 1})\"\n",
    "        print(f\"  Class {cls:2d} ({cls_type:20s}): {cls_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-title",
   "metadata": {},
   "source": [
    "## 7. 訓練結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Episode reward\n",
    "axes[0].plot(episode_rewards)\n",
    "axes[0].set_title('Episode Rewards')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Total Reward')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(episode_losses)\n",
    "axes[1].set_title('Training Loss')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Test accuracy\n",
    "if episode_accuracies:\n",
    "    episodes, accs = zip(*episode_accuracies)\n",
    "    axes[2].plot(episodes, accs, marker='o')\n",
    "    axes[2].set_title('Test Accuracy')\n",
    "    axes[2].set_xlabel('Episode')\n",
    "    axes[2].set_ylabel('Accuracy')\n",
    "    axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-save-title",
   "metadata": {},
   "source": [
    "## 8. モデルの保存（オプション）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練済みモデルの保存\n",
    "save_path = CHECKPOINT_DIR / f\"rl_session{TARGET_SESSION}_dqn.pth\"\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'policy_net_state_dict': policy_net.state_dict(),\n",
    "    'target_net_state_dict': target_net.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'session': TARGET_SESSION,\n",
    "    'test_accuracy': final_test_acc,\n",
    "    'hyperparameters': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'gamma': GAMMA,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'epsilon_decay': EPSILON_DECAY,\n",
    "    }\n",
    "}, save_path)\n",
    "\n",
    "print(f\"✓ Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-notes-title",
   "metadata": {},
   "source": [
    "## 使用方法とカスタマイズ\n",
    "\n",
    "### 設定の変更\n",
    "- `TARGET_SESSION`: 訓練したいセッション番号（1以降）\n",
    "- `freeze_encoder=True`: ベースモデルのエンコーダを凍結して新しい分類ヘッドのみ訓練\n",
    "- `NUM_EPISODES`, `LEARNING_RATE`: RL訓練のハイパーパラメータ調整\n",
    "\n",
    "### 複数セッションの連続訓練\n",
    "```python\n",
    "for session in range(1, NUM_SESSIONS + 1):\n",
    "    TARGET_SESSION = session\n",
    "    # 上記のコードを実行\n",
    "    # 各セッション終了後にモデルを保存\n",
    "```\n",
    "\n",
    "### 性能改善のヒント\n",
    "1. **エンコーダの凍結**: `freeze_encoder=True`で既存クラスの忘却を抑制\n",
    "2. **正則化**: L2正則化やDropoutを追加\n",
    "3. **リプレイバッファ**: 古いクラスのサンプルも保存して訓練\n",
    "4. **Prioritized Experience Replay**: 重要なサンプルを優先的にサンプリング"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
